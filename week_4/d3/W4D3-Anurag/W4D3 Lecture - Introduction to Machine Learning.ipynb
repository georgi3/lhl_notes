{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics\n",
    "* Introduction to Machine Learning\n",
    "* Different types of Machine Learning Applications\n",
    "* Empirical Risk Minimization\n",
    "* Errors in Prediction\n",
    "* Approximation error\n",
    "* Estimation error\n",
    "* Optimization error\n",
    "* Linear Regression\n",
    "* Risk minimization\n",
    "* Computing weights\n",
    "* Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "The majority of the focus should be on:\n",
    "\n",
    "In the beginning, we should spend some time on generic stuff, AI vs. ML vs. Deep Learning, introduction, comparison with Statistics, different types of ML, applications (20 min)\n",
    "\n",
    "Empirical Risk Minimization - differences between Approximation Error, Estimation Error, and Optimization Error. Why they occur and how to minimize them (30 min).\n",
    "\n",
    "Linear Regression - computing weights. Explain (proof) the least-squares method. (30 min)\n",
    "\n",
    "Polynomial Regression (10 min) - show overfitting/underfitting in graphs. This is the first time they will see this, so only basic graphics. We will also cover this next Monday in a lecture on Evaluation.\n",
    "\n",
    "Show Regression Results in Python (30 min), statsmodel better than sklearn. Show interpretation of coefficients, p-values (refer to hypothesis testing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI vs ML vs Deep Learning (20 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s clear things up: artificial intelligence (AI), machine learning (ML), and deep learning (DL) are three different things.\n",
    "\n",
    "*<b>Artificial intelligence</b> is a science like mathematics or biology. It studies ways to build intelligent programs and machines that can creatively solve problems, which has always been considered a human prerogative.\n",
    "\n",
    "*<b>Machine learning</b> is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. In ML, there are different algorithms (e.g. neural networks) that help to solve problems.\n",
    "    \n",
    "*<b>Deep learning</b>, or deep neural learning, is a subset of machine learning, which uses the neural networks to analyze different factors with a structure that is similar to the human neural system.\n",
    "\n",
    "[source: 'https://medium.com/ai-in-plain-english/artificial-intelligence-vs-machine-learning-vs-deep-learning-whats-the-difference-dccce18ef']\n",
    "\n",
    "<img src='./deeplearning-ml-ai.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of AI\n",
    "* <b>Weak AI</b>:Weak, or narrow AI, is good at performing a particular task, but it will not pass for human in any other field outside of its defined capacities.famous example of AI beating humans in games is AlphaGo. This program won in one of the most complicated games ever invented, learning how to play it and not just calculating all the possible moves (which is impossible).\n",
    "\n",
    "* <b>Strong AI</b>:This is the point in the future when machines become human-like. They make their own decisions and learn without any human input. Not only are they competent in solving logical tasks but they also have emotions. Machine Reasoning is a field under research currently which may lead to strong AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do machines learn?\n",
    "Machine Learning involves using algorithms that train on labelled data and predict target variables. For ML you need Datasets,Features and an algorithm as shown below.\n",
    "<img src='ML1.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<b>Datasets</b>: Machine learning systems are trained on special collections of samples called datasets. The samples can include numbers, images, texts or any other kind of data. It usually takes a lot of time and effort to create a good dataset. Discover more about data preparation for machine learning here.\n",
    "\n",
    "*<b>Features</b>: Features are important pieces of data that work as the key to the solution of the task. They demonstrate to the machine what to pay attention to. How do you select the features? Let’s say, you want to predict the price of an apartment. It is hard to predict by linear regression how much the place can cost based on the combination of its length and width, for example. However, it is much easier to find a correlation between the price and the area where the building is located.\n",
    "\n",
    "*<b>Algorithm</b>: It is possible to solve the same task using different algorithms. Depending on the algorithm, the accuracy or speed of getting the results can be different. Sometimes in order to achieve better performance, you combine different algorithms, like in ensemble learning.\n",
    "<img src='ML2.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning\n",
    "Deep learning is a class of machine learning algorithms inspired by the structure of a human brain. Deep learning algorithms use complex multi-layered neural networks, where the level of abstraction increases gradually by non-linear transformations of input data.\n",
    "<img src='DL.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Risk Minimization\n",
    "[source: 'https://towardsdatascience.com/learning-theory-empirical-risk-minimization-d3573f90ff77']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote the domain space with <b>X</b> and the label space with <b>Y</b>, we also need a function for mapping the domain set space to the label set space, <b>f: X -> Y</b>, this is just a formal definition of a learning task.\n",
    "\n",
    "Now that we have our formal problem definition, we need a model that is going to make our predictions: spam or not spam. Coincidentally, the synonym for <b>model</b> is the hypothesis <b>h</b>, which may be a bit confusing. The hypothesis, in this case, is nothing else than a function which takes input from our domain <b>X</b> and produces a <b>label 0 or 1</b>, i.e. a function <b>h: X -> Y</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the end, we actually want to find the hypothesis that minimizes our error With this, we come to the term empirical risk minimization. \n",
    "\n",
    "* The term empirical implies that we minimize our error based on a sample set S from the domain set X. Looking at it from a probabilistic perspective we say that we sample S from the domain set X, with D being the distribution over X. So when we sample from the domain, we express how likely a subset of the domain is sampled from the domain X by D(S).\n",
    "\n",
    "* Since we only have access to S, a subset of the input domain, we learn based on that sample of training examples. We don’t have access to the true error, but to the empirical error:\n",
    "<img src='emr-diag1.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximation Error and Estimation Error\n",
    "So the ERM paradigm find the predictor of your set of hypothesis that minimize the Empirical error. Now taking the ERM predictor hs, we can decompose the test error of it in two parts:\n",
    "\n",
    "<img src = './EMR3.jpg'>\n",
    "\n",
    "The first term is the approximation error and intuitively, the richer our class of hypotheses the lower is this error. \n",
    "\n",
    "But on the other hand the richer our class of hypotheses the higher is the estimation error, so there is this bias-complexity trade-off. \n",
    "\n",
    "Intuitively if we have many many predictors in our hypothesis class we will have a greater probability of finding the best predictor with the ERM algorithm; we will find an excellent predictor that perfectly describes the data of the training set, therefore, what we are doing is overestimating our problem, ie we are doing overfitting. \n",
    "\n",
    "On the other hand, if we restrict our hypothesis class to much we are underestimating our problem and we are doing underfitting, so that’s the bias trade-off.\n",
    "\n",
    "<img src = './EMR4.jpg'>\n",
    "\n",
    "[source: https://medium.com/@francescomandru/the-power-of-boosting-in-ml-3bfdf4ab9806 ]\n",
    "\n",
    "#### To Summarize:\n",
    "The hypothesis function you selected which has minimal risk will have 2 errors:\n",
    "1. approximation error\n",
    "2. estimation error\n",
    "3. balancing these 2 errors will help in achieving a better generalized model which performs okay on all data(seen and unseen).This is also known as the <b> Bias-Variance Tradeoff</b>\n",
    "\n",
    "#### Further Reading:\n",
    "https://www.quora.com/How-are-estimation-and-approximation-related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Machine Learning:\n",
    "<img src='./regression-vs-classification-in-machine-learning.png'>\n",
    "\n",
    "<img src='./supvsunsup.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Y = target variable(dependent variable)\n",
    "X = Features ( independent variables)\n",
    "\n",
    "We try to find relationship between X and Y using the equation Y = mX+c. Basically we try to find the right values for m and C which help us predict maximum points correctly with minimum error.\n",
    "\n",
    "<img src='./linreg1.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Linear Regression\n",
    "y = β0 + β1x1 + β2x2 + c\n",
    "\n",
    "y = predicted variable\n",
    "\n",
    "x1,x2,c = features. c is bias\n",
    "\n",
    "When you have multiple features or independent variables and a single dependent or target variable. We need to find correct weights/coefficients ( betas in this case) for all these features to find the best approximator or predict most points correctly this is known as multiple linear regression.\n",
    "\n",
    "<img src = './multiplelinearregression.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity: Go through the link and understand it so we can discuss it.\n",
    "https://setosa.io/ev/ordinary-least-squares-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS Regression vs Linear Regression\n",
    "OLS regression is a version of Linear Regression. Linear regression uses gradient descent which is an iterative form of finding parameters whereas OLS uses least squares and uses a formula to calculate the parameters rather than iterating over n examples in a dataset.\n",
    "\n",
    "OLS does not use an objective(cost) function which is used by Linear Regression. OLS is solved using system of equations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression using statsmodels\n",
    "[material source: 'https://datatofish.com/statsmodels-linear-regression/#:~:text=Linear%20regression%20is%20used%20as,s%20used%20in%20the%20prediction).' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python code includes an example of Multiple Linear Regression, where the input variables are:\n",
    "\n",
    "* Interest_Rate\n",
    "* Unemployment_Rate\n",
    "\n",
    "These two variables are used in the prediction of the dependent variable of Stock_Index_Price.\n",
    "\n",
    "Here is the complete syntax to perform the linear regression in Python using statsmodels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:      Stock_Index_Price   R-squared:                       0.898\n",
      "Model:                            OLS   Adj. R-squared:                  0.888\n",
      "Method:                 Least Squares   F-statistic:                     92.07\n",
      "Date:                Wed, 27 Jan 2021   Prob (F-statistic):           4.04e-11\n",
      "Time:                        09:16:39   Log-Likelihood:                -134.61\n",
      "No. Observations:                  24   AIC:                             275.2\n",
      "Df Residuals:                      21   BIC:                             278.8\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const              1798.4040    899.248      2.000      0.059     -71.685    3668.493\n",
      "Interest_Rate       345.5401    111.367      3.103      0.005     113.940     577.140\n",
      "Unemployment_Rate  -250.1466    117.950     -2.121      0.046    -495.437      -4.856\n",
      "==============================================================================\n",
      "Omnibus:                        2.691   Durbin-Watson:                   0.530\n",
      "Prob(Omnibus):                  0.260   Jarque-Bera (JB):                1.551\n",
      "Skew:                          -0.612   Prob(JB):                        0.461\n",
      "Kurtosis:                       3.226   Cond. No.                         394.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pandas import DataFrame\n",
    "import statsmodels.api as sm\n",
    "\n",
    "Stock_Market = {'Year': [2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016],\n",
    "                'Month': [12, 11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1],\n",
    "                'Interest_Rate': [2.75,2.5,2.5,2.5,2.5,2.5,2.5,2.25,2.25,2.25,2,2,2,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75],\n",
    "                'Unemployment_Rate': [5.3,5.3,5.3,5.3,5.4,5.6,5.5,5.5,5.5,5.6,5.7,5.9,6,5.9,5.8,6.1,6.2,6.1,6.1,6.1,5.9,6.2,6.2,6.1],\n",
    "                'Stock_Index_Price': [1464,1394,1357,1293,1256,1254,1234,1195,1159,1167,1130,1075,1047,965,943,958,971,949,884,866,876,822,704,719]        \n",
    "                }\n",
    "\n",
    "df = DataFrame(Stock_Market,columns=['Year','Month','Interest_Rate','Unemployment_Rate','Stock_Index_Price']) \n",
    "\n",
    "X = df[['Interest_Rate','Unemployment_Rate']] # here we have 2 variables for the multiple linear regression. If you just want to use one variable for simple linear regression, then use X = df['Interest_Rate'] for example\n",
    "Y = df['Stock_Index_Price']\n",
    "\n",
    "X = sm.add_constant(X) # adding a constant\n",
    "\n",
    "model = sm.OLS(Y, X).fit()\n",
    "predictions = model.predict(X) \n",
    "\n",
    "print_model = model.summary()\n",
    "print(print_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the Regression Results:\n",
    "\n",
    "* <b>Adjusted. R-squared</b> reflects the fit of the model. R-squared values range from 0 to 1, where a higher value generally indicates a better fit, assuming certain conditions are met.\n",
    "\n",
    "* <b>const coefficient</b> is your Y-intercept. It means that if both the Interest_Rate and Unemployment_Rate coefficients are zero, then the expected output (i.e., the Y) would be equal to the const coefficient.\n",
    "\n",
    "* <b>Interest_Rate</b> coefficient represents the change in the output Y due to a change of one unit in the interest rate (everything else held constant)\n",
    "\n",
    "* <b>Unemployment_Rate coefficient</b> represents the change in the output Y due to a change of one unit in the unemployment rate (everything else held constant)\n",
    "\n",
    "* <b>std err</b> reflects the level of accuracy of the coefficients. The lower it is, the higher is the level of accuracy\n",
    "\n",
    "* <b>P >|t| is your p-value</b>. A p-value of less than 0.05 is considered to be statistically significant\n",
    "\n",
    "* <b>Confidence Interval</b> represents the range in which our coefficients are likely to fall (with a likelihood of 95%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
